{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457d1a35",
   "metadata": {},
   "source": [
    "A toy class encapsulating some of the difficulties of yt's existing io handling (for reading from a gadget hdf5 file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2fa0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import h5py\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class FileHandler:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = os.path.expanduser(filename)\n",
    "        \n",
    "    def read_field(self, hdf_keys: List[str]):        \n",
    "        with h5py.File(self.filename, \"r\") as f:\n",
    "            contents = np.array(f[\"/\".join(hdf_keys)])\n",
    "        return contents\n",
    "    \n",
    "    def read_fields_bad(self, particle_type: str, field_list: List[str]) -> dict:\n",
    "        fields = {}\n",
    "        for field in field_list:\n",
    "            # this opens/closes file every read!\n",
    "            fields[field] = read_field([particle_type, field])\n",
    "\n",
    "        return fields\n",
    "    \n",
    "    def read_fields_good(self, particle_type: str, field_list: List[str]) -> dict:\n",
    "        fields = {}\n",
    "        # opens the file once, but duplicates code in read_field\n",
    "        with h5py.File(self.filename, \"r\") as f:\n",
    "            for field in field_list:                \n",
    "                fields[field] = np.array(f[f\"{particle_type}/{field}\"])\n",
    "\n",
    "        return fields    \n",
    "    \n",
    "    def read_smoothing_length(self, particle_type):\n",
    "        with h5py.File(self.filename, \"r\") as f:\n",
    "            hsml = np.array(f[f\"{particle_type}/SmoothingLength\"])\n",
    "        return hsml         \n",
    "    \n",
    "    def read_coordinates(self, particle_type: str):\n",
    "        with h5py.File(self.filename, \"r\") as f:\n",
    "            xyz = np.array(f[f\"{particle_type}/Coordinates\"])\n",
    "            hsml = 0.\n",
    "            if particle_type == \"PartType0\":\n",
    "                # this double reads! \n",
    "                hsml = self.read_smoothing_length(particle_type)\n",
    "                \n",
    "        return xyz, hsml\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9a283",
   "metadata": {},
   "source": [
    "The above class nicely encapsulates how to read a single field from an hdf file with the `read_field` method, but when using that method to read multiple fields (like in `read_fields_bad`), it would require opening and closing the file multiple times. To avoid this, we could write a new method, `read_fields_good`, where the new method opens and closes the file handle explicitly. This results in code duplication, and if not careful can lead to some less than ideal situations. \n",
    "\n",
    "In yt, we have many convenience functions for pulling certain data columns from disk. In the above class, the `read_smoothing_length` method mimics one of yt's commonly implemented io functions. But in yt, we also return the smoothing length whenever we read the coordinates, and so it's tempting to write (as in the above):\n",
    "\n",
    "```python\n",
    "    def read_coordinates(self, particle_type: str):\n",
    "        with h5py.File(self.filename, \"r\") as f:\n",
    "            xyz = np.array(f[f\"{particle_type}/Coordinates\"])\n",
    "            hsml = 0.\n",
    "            if particle_type == \"PartType0\":\n",
    "                # this double reads! \n",
    "                hsml = self.read_smoothing_length(particle_type)\n",
    "```\n",
    "\n",
    "This, however, would re-open an already open file! The above example is fairly trivial to fix -- we could just copy the internals of `read_smoothing_length` up to `read_coordinates`. But that results in yet more code duplication and in the case of the real yt example, it's actually not quite so easy to re-write it in this way (the real functions are more complex). \n",
    "\n",
    "So ideally, we want to be able to:\n",
    "\n",
    "* encapsulate and re-use the most basic file operation (fetching data from disk)\n",
    "* avoid unnecessarily opening/closing files\n",
    "* avoid opening already opened files\n",
    "* minimize code duplication\n",
    "\n",
    "One way to do this is with a nested `@contextlib.contextmanager`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32af4ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import h5py\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class FileHandler:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = os.path.expanduser(filename)\n",
    "        \n",
    "    @contextlib.contextmanager\n",
    "    def transaction(self, handle = None):\n",
    "        if handle is None:\n",
    "            with self.open_handle() as handle:\n",
    "                yield handle\n",
    "        else:            \n",
    "            yield handle\n",
    "        \n",
    "    @contextlib.contextmanager\n",
    "    def open_handle(self):\n",
    "        f = h5py.File(self.filename, \"r\")\n",
    "        yield f\n",
    "        f.close()\n",
    "        \n",
    "    def read_field(self, hdf_keys: List[str], handle=None):        \n",
    "        with self.transaction(handle) as f:\n",
    "            contents = np.array(f[\"/\".join(hdf_keys)])\n",
    "        return contents\n",
    "    \n",
    "    def read_fields(self, particle_type: str, field_list: List[str], handle=None) -> dict:\n",
    "        fields = {}\n",
    "        with self.transaction(handle) as f:\n",
    "            for field in field_list:\n",
    "                fields[field] = self.read_field([particle_type, field], handle=f)\n",
    "        return fields\n",
    "    \n",
    "    def read_smoothing_length(self, particle_type, handle=None):\n",
    "        with self.transaction(handle) as f:\n",
    "            hsml = self.read_field([particle_type, \"SmoothingLength\"], handle=f)\n",
    "        return hsml         \n",
    "    \n",
    "    def read_coordinates(self, particle_type: str, handle=None):\n",
    "        with self.transaction(handle) as f:\n",
    "            xyz = self.read_field([particle_type, \"Coordinates\"], handle=f)\n",
    "            hsml = 0.\n",
    "            if particle_type == \"PartType0\":\n",
    "                # this no longer double reads! \n",
    "                hsml = self.read_smoothing_length(particle_type, handle=f)\n",
    "                \n",
    "        return xyz, hsml\n",
    "    \n",
    "    def read_fields_with_coords(self, particle_type: str, fields: List[str]):\n",
    "        \n",
    "        with self.transaction() as f:\n",
    "            field_data = self.read_fields(particle_type, fields, handle=f)\n",
    "            coords, hsml = self.read_coordinates(particle_type, handle=f)\n",
    "            \n",
    "        return field_data, coords, hsml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c3d04",
   "metadata": {},
   "source": [
    "The above construction creates a recursive `transaction` generator. If you don't give it any arguments, it yields a new file handle that you can use in a typical `with` constructor. But if you pass in an existing handle, it will simply yield that handle. The benefit of this construction is that you can build very flexible methods that rely on tightly constrained behavior without re-opening files. Consider `read_fields_with_coords`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1b987fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Density': array([ 6577205. , 15850306. ,  6765328.5, ...,  6816981. , 22548702. ,\n",
       "         25834210. ], dtype=float32)},\n",
       " array([[ 7.6320577 , 11.81454   ,  0.5112596 ],\n",
       "        [ 7.630863  , 11.814384  ,  0.51114064],\n",
       "        [ 7.633304  , 11.81966   ,  0.51152855],\n",
       "        ...,\n",
       "        [ 9.948605  ,  8.47677   , 14.566635  ],\n",
       "        [ 9.948661  ,  8.478258  , 14.567051  ],\n",
       "        [ 9.94791   ,  8.478077  , 14.566901  ]], dtype=float32),\n",
       " array([0.00320586, 0.00230037, 0.00324003, ..., 0.00309351, 0.00218892,\n",
       "        0.00213684], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_handler = FileHandler(\"~/hdd/data/yt_data/yt_sample_sets/snapshot_033/snap_033.0.hdf5\")\n",
    "\n",
    "field_data, xyz, hsml = file_handler.read_fields_with_coords('PartType0', ['Density'])\n",
    "field_data, xyz, hsml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e876e",
   "metadata": {},
   "source": [
    "In `read_fields_with_coords`, we open a file handle and pass that handle down -- that handle gets passed down all the way to the base `read_field` call, where we define the actual file-specific method for reading off of disk. Every other method is a derective for reading data in different ways, but we only define the actual file specific behavior in a single spot. This allows us to construct methods by combining any of the existing methods without worrying about whether or not our file is already open. Furthermore, it allows us to extend this class to other file types easily -- simply swap out how the file is opened in `open_handle` (and abstract away some of the above gadget-specific conventions, like the specification of `'Coordinates'`)! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eef7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
