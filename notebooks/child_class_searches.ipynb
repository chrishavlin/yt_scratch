{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "607d42de-004b-4550-9365-33a19bdfbc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "import inspect\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from yt.utilities.io_handler import BaseIOHandler\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7596b0d-25f9-40ca-92b5-d30dd3631317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e01d7589004d0692c33e7f64e81f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(options=(('BaseParticleIOHandler', <class 'yt.utilities.io_handler.BaseParticleIOHandl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_subclasses = []\n",
    "def subclasses_assemble(parent):\n",
    "    for child in parent.__subclasses__(): \n",
    "        all_subclasses.append(child)  # add the name        \n",
    "        subclasses_assemble(child)  # continue downward... \n",
    "    \n",
    "subclasses_assemble(BaseIOHandler)\n",
    "\n",
    "class_dropdown = ipywidgets.Dropdown(options=[(_.__name__, _) for _ in all_subclasses])\n",
    "func_dropdown = ipywidgets.Dropdown(options=[_ for _ in dir(BaseIOHandler) if not _.startswith(\"__\")])\n",
    "defined_at = ipywidgets.HTML()\n",
    "source = ipywidgets.Output(layout=ipywidgets.Layout(width=\"50%\", height=\"50em\"))\n",
    "\n",
    "\n",
    "def update_class(event):\n",
    "    current_func = func_dropdown.value\n",
    "    func_dropdown.options = [_ for _ in dir(class_dropdown.value) if not _.startswith(\"__\")]\n",
    "    if current_func in func_dropdown.options:\n",
    "        func_dropdown.value = current_func\n",
    "\n",
    "update_class(None)\n",
    "class_dropdown.observe(update_class, [\"value\"])\n",
    "    \n",
    "def update_source(event):\n",
    "    cls = class_dropdown.value\n",
    "    f = getattr(cls, func_dropdown.value)\n",
    "    \n",
    "    source.clear_output()\n",
    "    if not isinstance(f, collections.abc.Callable): return\n",
    "    defined_at.value = f\"<tt>{inspect.getsourcefile(f)}:{inspect.getsourcelines(f)[1]}</tt>\"\n",
    "    with source:\n",
    "        display(\n",
    "            Markdown(\n",
    "                data=\"```python\\n\"\n",
    "                + textwrap.dedent(inspect.getsource(f))\n",
    "                + \"\\n```\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "func_dropdown.observe(update_source, [\"value\"])\n",
    "class_dropdown.observe(update_class, [\"value\"])\n",
    "update_source(None)\n",
    "display(ipywidgets.VBox([class_dropdown, func_dropdown, defined_at, source]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31eb81-5431-40b9-ace0-61315b1f9279",
   "metadata": {},
   "source": [
    "On only display those children with functions that differ from the base handler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c6642b4-c334-4eb0-bcd0-5889b1b5ea7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[yt.frontends.gadget_fof.io.IOHandlerGadgetFOFHaloHDF5,\n",
       " yt.frontends.halo_catalog.io.IOHandlerYTHalo,\n",
       " yt.frontends.open_pmd.io.IOHandlerOpenPMDHDF5]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def source_check(obj, func_name):\n",
    "    f = getattr(obj, func_name)\n",
    "    if isinstance(f, collections.abc.Callable):         \n",
    "        return f\"{inspect.getsourcefile(f)}:{inspect.getsourcelines(f)[1]}\"\n",
    "    return\n",
    "    \n",
    "def child_overrides_func(func_name, child, parent):\n",
    "    c_func = source_check(child, func_name)\n",
    "    p_func = source_check(parent, func_name)\n",
    "    return c_func != p_func\n",
    "\n",
    "def get_children_classes(base_class, func_name=None):\n",
    "    # recursively builds a list of all children that override func_name of base_class\n",
    "    \n",
    "    all_subclasses = []     \n",
    "    def subclasses_assemble(parent):\n",
    "        for child in parent.__subclasses__(): \n",
    "            if func_name is None or child_overrides_func(func_name, child, base_class):\n",
    "                all_subclasses.append(child)  # add the name        \n",
    "            subclasses_assemble(child)  # continue downward... \n",
    "\n",
    "    subclasses_assemble(base_class) # all the children\n",
    "    return all_subclasses\n",
    "\n",
    "\n",
    "subclasses = get_children_classes(BaseIOHandler, '_read_particle_selection')\n",
    "subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9ec10de-1656-4924-85bb-b845d200a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_funcs = [_ for _ in dir(BaseIOHandler) if not _.startswith(\"__\")]\n",
    "\n",
    "subclasses_that_override = {}\n",
    "for f in base_funcs:\n",
    "    subclasses_that_override[f] = get_children_classes(BaseIOHandler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f2a2708-345e-4329-9ca2-c304bd5eb29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[yt.frontends.gadget_fof.io.IOHandlerGadgetFOFHaloHDF5,\n",
       " yt.frontends.halo_catalog.io.IOHandlerYTHalo,\n",
       " yt.frontends.open_pmd.io.IOHandlerOpenPMDHDF5]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subclasses_that_override['_read_particle_selection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa1bf559-5e34-4225-a7c8-25e365a79db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae214b923184b228296d3d7437a07fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(index=18, options=('_cache_on', '_count_particles_chunks', '_count_selected_particles'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "func_dropdown = ipywidgets.Dropdown(options=base_funcs, value='_read_particle_selection')\n",
    "class_dropdown = ipywidgets.Dropdown(options=subclasses_that_override[func_dropdown.value])\n",
    "defined_at = ipywidgets.HTML()\n",
    "source = ipywidgets.Output(layout=ipywidgets.Layout(width=\"100%\", height=\"50em\"))\n",
    "\n",
    "\n",
    "def update_classlist(event):\n",
    "    # function value has changed, update the class\n",
    "    current_func = func_dropdown.value\n",
    "    class_dropdown.options = subclasses_that_override[current_func]\n",
    "\n",
    "update_classlist(None)\n",
    "func_dropdown.observe(update_classlist, [\"value\"])\n",
    "\n",
    "   \n",
    "def update_source(event):\n",
    "    cls = class_dropdown.value\n",
    "    if cls is None: \n",
    "        source.clear_output()\n",
    "        defined_at.value = \"\"\n",
    "        return\n",
    "    f = getattr(cls, func_dropdown.value)\n",
    "    \n",
    "    source.clear_output()\n",
    "    if not isinstance(f, collections.abc.Callable): return\n",
    "    defined_at.value = f\"<tt>{inspect.getsourcefile(f)}:{inspect.getsourcelines(f)[1]}</tt>\"\n",
    "    with source:\n",
    "        display(\n",
    "            Markdown(\n",
    "                data=\"```python\\n\"\n",
    "                + textwrap.dedent(inspect.getsource(f))\n",
    "                + \"\\n```\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "func_dropdown.observe(update_classlist, [\"value\"])  # if selected function changes, update the class list\n",
    "class_dropdown.observe(update_source, [\"value\"])  # if selected class changes, update the source\n",
    "update_source(None)\n",
    "display(ipywidgets.VBox([func_dropdown, class_dropdown, defined_at, source]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6a06027-c1b7-420e-88fc-bcd636e41ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "-     def _read_particle_selection(\n",
       "+     def _read_particle_selection(self, dobj, fields):\n",
       "?                                  ++++++++++++++++++++\n",
       "\n",
       "+         rv = {}\n",
       "+         ind = {}\n",
       "-         self, chunks, selector, fields: List[Tuple[str, str]]\n",
       "-     ) -> Mapping[Tuple[str, str], np.ndarray]:\n",
       "-         rv = {}  # the return dictionary\n",
       "-         ind = {}  # holds the most recent max index of the return arrays by field\n",
       "- \n",
       "-         # Initialize containers for tracking particle, field information\n",
       "-         # ptf (particle field types) maps particle type to list of on-disk fields to read\n",
       "-         # psize maps particle type to on-disk size across chunks\n",
       "-         # fsize maps particle type to size of return values\n",
       "-         # field_maps stores fields, accounting for field unions\n",
       "-         ptf: DefaultDict[str, List[str]] = defaultdict(list)\n",
       "-         psize: DefaultDict[str, int] = defaultdict(lambda: 0)\n",
       "-         fsize: DefaultDict[Tuple[str, str], int] = defaultdict(lambda: 0)\n",
       "-         field_maps: DefaultDict[Tuple[str, str], List[Tuple[str, str]]] = defaultdict(\n",
       "-             list\n",
       "-         )\n",
       "- \n",
       "          # We first need a set of masks for each particle type\n",
       "-         chunks = list(chunks)\n",
       "+         ptf = defaultdict(list)  # ON-DISK TO READ\n",
       "+         fsize = defaultdict(lambda: 0)  # COUNT RV\n",
       "+         field_maps = defaultdict(list)  # ptypes -> fields\n",
       "          unions = self.ds.particle_unions\n",
       "          # What we need is a mapping from particle types to return types\n",
       "          for field in fields:\n",
       "              ftype, fname = field\n",
       "              fsize[field] = 0\n",
       "              # We should add a check for p.fparticle_unions or something here\n",
       "              if ftype in unions:\n",
       "                  for pt in unions[ftype]:\n",
       "                      ptf[pt].append(fname)\n",
       "                      field_maps[pt, fname].append(field)\n",
       "              else:\n",
       "                  ptf[ftype].append(fname)\n",
       "                  field_maps[field].append(field)\n",
       "-         # Now we have our full listing\n",
       "- \n",
       "-         # Now we add particle counts across chunks to psize\n",
       "-         self._count_particles_chunks(psize, chunks, ptf, selector)\n",
       "\n",
       "          # Now we allocate\n",
       "+         psize = {dobj.ptype: dobj.particle_number}\n",
       "          for field in fields:\n",
       "              if field[0] in unions:\n",
       "                  for pt in unions[field[0]]:\n",
       "                      fsize[field] += psize.get(pt, 0)\n",
       "              else:\n",
       "                  fsize[field] += psize.get(field[0], 0)\n",
       "-         shape: Tuple[int, ...]\n",
       "          for field in fields:\n",
       "              if field[1] in self._vector_fields:\n",
       "-                 vsize = self._vector_fields[field[1]]  # type:ignore\n",
       "-                 # note: the above line causes a mypy failure due to how we\n",
       "-                 # convert _vector_fields from a tuple to dict in __init__. mypy\n",
       "-                 # is expecting a tuple here. And since _vector_fields is used in\n",
       "-                 # many places, just ignoring for now...\n",
       "-                 shape = (fsize[field], vsize)\n",
       "?                                           -\n",
       "\n",
       "+                 shape = (fsize[field], self._vector_fields[field[1]])\n",
       "?                                        ++++++ +++++++++++ ++  ++++++\n",
       "\n",
       "              elif field[1] in self._array_fields:\n",
       "                  shape = (fsize[field],) + self._array_fields[field[1]]\n",
       "+             elif field in self.ds.scalar_field_list:\n",
       "+                 shape = (1,)\n",
       "              else:\n",
       "                  shape = (fsize[field],)\n",
       "              rv[field] = np.empty(shape, dtype=\"float64\")\n",
       "              ind[field] = 0\n",
       "          # Now we read.\n",
       "-         for field_r, vals in self._read_particle_fields(chunks, ptf, selector):\n",
       "?                                                         ^^^^^^     ----------\n",
       "\n",
       "+         for field_r, vals in self._read_particle_fields(dobj, ptf):\n",
       "?                                                         ^^^^\n",
       "\n",
       "              # Note that we now need to check the mappings\n",
       "              for field_f in field_maps[field_r]:\n",
       "                  my_ind = ind[field_f]\n",
       "-                 # mylog.debug(\"Filling %s from %s to %s with %s\",\n",
       "-                 #    field_f, my_ind, my_ind+vals.shape[0], field_r)\n",
       "                  rv[field_f][my_ind : my_ind + vals.shape[0], ...] = vals\n",
       "                  ind[field_f] += vals.shape[0]\n",
       "          # Now we need to truncate all our fields, since we allow for\n",
       "          # over-estimating.\n",
       "          for field_f in ind:\n",
       "              rv[field_f] = rv[field_f][: ind[field_f]]\n",
       "          return rv\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "from pprint import pprint\n",
    "\n",
    "basefunc = getattr(BaseIOHandler, \"_read_particle_selection\")\n",
    "func_src = inspect.getsource(basefunc).split(\"\\n\")\n",
    "\n",
    "cfunc = getattr(yt.frontends.gadget_fof.IOHandlerGadgetFOFHaloHDF5, \"_read_particle_selection\")\n",
    "f_c = inspect.getsource(cfunc).split(\"\\n\")\n",
    "d = difflib.Differ()\n",
    "result = list(d.compare(func_src, f_c))\n",
    "diffresult = \"\\n\".join(result)\n",
    "\n",
    "Markdown(\n",
    "                data=\"```python\\n\"\n",
    "                + textwrap.dedent(diffresult)\n",
    "                + \"\\n```\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "079d1328-7e00-41ce-9a19-be159af3f8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "199a6fb5-8e70-4325-b617-9b1d077224bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-     def _read_particle_selection(',\n",
      " '+     def _read_particle_selection(self, dobj, fields):',\n",
      " '?                                  ++++++++++++++++++++\\n',\n",
      " '+         rv = {}',\n",
      " '+         ind = {}',\n",
      " '-         self, chunks, selector, fields: List[Tuple[str, str]]',\n",
      " '-     ) -> Mapping[Tuple[str, str], np.ndarray]:',\n",
      " '-         rv = {}  # the return dictionary',\n",
      " '-         ind = {}  # holds the most recent max index of the return arrays '\n",
      " 'by field',\n",
      " '- ',\n",
      " '-         # Initialize containers for tracking particle, field information',\n",
      " '-         # ptf (particle field types) maps particle type to list of on-disk '\n",
      " 'fields to read',\n",
      " '-         # psize maps particle type to on-disk size across chunks',\n",
      " '-         # fsize maps particle type to size of return values',\n",
      " '-         # field_maps stores fields, accounting for field unions',\n",
      " '-         ptf: DefaultDict[str, List[str]] = defaultdict(list)',\n",
      " '-         psize: DefaultDict[str, int] = defaultdict(lambda: 0)',\n",
      " '-         fsize: DefaultDict[Tuple[str, str], int] = defaultdict(lambda: 0)',\n",
      " '-         field_maps: DefaultDict[Tuple[str, str], List[Tuple[str, str]]] = '\n",
      " 'defaultdict(',\n",
      " '-             list',\n",
      " '-         )',\n",
      " '- ',\n",
      " '          # We first need a set of masks for each particle type',\n",
      " '-         chunks = list(chunks)',\n",
      " '+         ptf = defaultdict(list)  # ON-DISK TO READ',\n",
      " '+         fsize = defaultdict(lambda: 0)  # COUNT RV',\n",
      " '+         field_maps = defaultdict(list)  # ptypes -> fields',\n",
      " '          unions = self.ds.particle_unions',\n",
      " '          # What we need is a mapping from particle types to return types',\n",
      " '          for field in fields:',\n",
      " '              ftype, fname = field',\n",
      " '              fsize[field] = 0',\n",
      " '              # We should add a check for p.fparticle_unions or something '\n",
      " 'here',\n",
      " '              if ftype in unions:',\n",
      " '                  for pt in unions[ftype]:',\n",
      " '                      ptf[pt].append(fname)',\n",
      " '                      field_maps[pt, fname].append(field)',\n",
      " '              else:',\n",
      " '                  ptf[ftype].append(fname)',\n",
      " '                  field_maps[field].append(field)',\n",
      " '-         # Now we have our full listing',\n",
      " '- ',\n",
      " '-         # Now we add particle counts across chunks to psize',\n",
      " '-         self._count_particles_chunks(psize, chunks, ptf, selector)',\n",
      " '  ',\n",
      " '          # Now we allocate',\n",
      " '+         psize = {dobj.ptype: dobj.particle_number}',\n",
      " '          for field in fields:',\n",
      " '              if field[0] in unions:',\n",
      " '                  for pt in unions[field[0]]:',\n",
      " '                      fsize[field] += psize.get(pt, 0)',\n",
      " '              else:',\n",
      " '                  fsize[field] += psize.get(field[0], 0)',\n",
      " '-         shape: Tuple[int, ...]',\n",
      " '          for field in fields:',\n",
      " '              if field[1] in self._vector_fields:',\n",
      " '-                 vsize = self._vector_fields[field[1]]  # type:ignore',\n",
      " '-                 # note: the above line causes a mypy failure due to how we',\n",
      " '-                 # convert _vector_fields from a tuple to dict in __init__. '\n",
      " 'mypy',\n",
      " '-                 # is expecting a tuple here. And since _vector_fields is '\n",
      " 'used in',\n",
      " '-                 # many places, just ignoring for now...',\n",
      " '-                 shape = (fsize[field], vsize)',\n",
      " '?                                           -\\n',\n",
      " '+                 shape = (fsize[field], self._vector_fields[field[1]])',\n",
      " '?                                        ++++++ +++++++++++ ++  ++++++\\n',\n",
      " '              elif field[1] in self._array_fields:',\n",
      " '                  shape = (fsize[field],) + self._array_fields[field[1]]',\n",
      " '+             elif field in self.ds.scalar_field_list:',\n",
      " '+                 shape = (1,)',\n",
      " '              else:',\n",
      " '                  shape = (fsize[field],)',\n",
      " '              rv[field] = np.empty(shape, dtype=\"float64\")',\n",
      " '              ind[field] = 0',\n",
      " '          # Now we read.',\n",
      " '-         for field_r, vals in self._read_particle_fields(chunks, ptf, '\n",
      " 'selector):',\n",
      " '?                                                         ^^^^^^     '\n",
      " '----------\\n',\n",
      " '+         for field_r, vals in self._read_particle_fields(dobj, ptf):',\n",
      " '?                                                         ^^^^\\n',\n",
      " '              # Note that we now need to check the mappings',\n",
      " '              for field_f in field_maps[field_r]:',\n",
      " '                  my_ind = ind[field_f]',\n",
      " '-                 # mylog.debug(\"Filling %s from %s to %s with %s\",',\n",
      " '-                 #    field_f, my_ind, my_ind+vals.shape[0], field_r)',\n",
      " '                  rv[field_f][my_ind : my_ind + vals.shape[0], ...] = vals',\n",
      " '                  ind[field_f] += vals.shape[0]',\n",
      " '          # Now we need to truncate all our fields, since we allow for',\n",
      " '          # over-estimating.',\n",
      " '          for field_f in ind:',\n",
      " '              rv[field_f] = rv[field_f][: ind[field_f]]',\n",
      " '          return rv',\n",
      " '  ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92d116f8-8dc7-4212-b2e4-42d847d414f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "-     def _read_particle_selection(\n",
       "+     def _read_particle_selection(self, dobj, fields):\n",
       "?                                  ++++++++++++++++++++\n",
       "\n",
       "+         rv = {}\n",
       "+         ind = {}\n",
       "-         self, chunks, selector, fields: List[Tuple[str, str]]\n",
       "-     ) -> Mapping[Tuple[str, str], np.ndarray]:\n",
       "-         rv = {}  # the return dictionary\n",
       "-         ind = {}  # holds the most recent max index of the return arrays by field\n",
       "- \n",
       "-         # Initialize containers for tracking particle, field information\n",
       "-         # ptf (particle field types) maps particle type to list of on-disk fields to read\n",
       "-         # psize maps particle type to on-disk size across chunks\n",
       "-         # fsize maps particle type to size of return values\n",
       "-         # field_maps stores fields, accounting for field unions\n",
       "-         ptf: DefaultDict[str, List[str]] = defaultdict(list)\n",
       "-         psize: DefaultDict[str, int] = defaultdict(lambda: 0)\n",
       "-         fsize: DefaultDict[Tuple[str, str], int] = defaultdict(lambda: 0)\n",
       "-         field_maps: DefaultDict[Tuple[str, str], List[Tuple[str, str]]] = defaultdict(\n",
       "-             list\n",
       "-         )\n",
       "- \n",
       "          # We first need a set of masks for each particle type\n",
       "-         chunks = list(chunks)\n",
       "+         ptf = defaultdict(list)  # ON-DISK TO READ\n",
       "+         fsize = defaultdict(lambda: 0)  # COUNT RV\n",
       "+         field_maps = defaultdict(list)  # ptypes -> fields\n",
       "          unions = self.ds.particle_unions\n",
       "          # What we need is a mapping from particle types to return types\n",
       "          for field in fields:\n",
       "              ftype, fname = field\n",
       "              fsize[field] = 0\n",
       "              # We should add a check for p.fparticle_unions or something here\n",
       "              if ftype in unions:\n",
       "                  for pt in unions[ftype]:\n",
       "                      ptf[pt].append(fname)\n",
       "                      field_maps[pt, fname].append(field)\n",
       "              else:\n",
       "                  ptf[ftype].append(fname)\n",
       "                  field_maps[field].append(field)\n",
       "-         # Now we have our full listing\n",
       "- \n",
       "-         # Now we add particle counts across chunks to psize\n",
       "-         self._count_particles_chunks(psize, chunks, ptf, selector)\n",
       "\n",
       "          # Now we allocate\n",
       "+         psize = {dobj.ptype: dobj.particle_number}\n",
       "          for field in fields:\n",
       "              if field[0] in unions:\n",
       "                  for pt in unions[field[0]]:\n",
       "                      fsize[field] += psize.get(pt, 0)\n",
       "              else:\n",
       "                  fsize[field] += psize.get(field[0], 0)\n",
       "-         shape: Tuple[int, ...]\n",
       "          for field in fields:\n",
       "              if field[1] in self._vector_fields:\n",
       "-                 vsize = self._vector_fields[field[1]]  # type:ignore\n",
       "-                 # note: the above line causes a mypy failure due to how we\n",
       "-                 # convert _vector_fields from a tuple to dict in __init__. mypy\n",
       "-                 # is expecting a tuple here. And since _vector_fields is used in\n",
       "-                 # many places, just ignoring for now...\n",
       "-                 shape = (fsize[field], vsize)\n",
       "?                                           -\n",
       "\n",
       "+                 shape = (fsize[field], self._vector_fields[field[1]])\n",
       "?                                        ++++++ +++++++++++ ++  ++++++\n",
       "\n",
       "              elif field[1] in self._array_fields:\n",
       "                  shape = (fsize[field],) + self._array_fields[field[1]]\n",
       "+             elif field in self.ds.scalar_field_list:\n",
       "+                 shape = (1,)\n",
       "              else:\n",
       "                  shape = (fsize[field],)\n",
       "              rv[field] = np.empty(shape, dtype=\"float64\")\n",
       "              ind[field] = 0\n",
       "          # Now we read.\n",
       "-         for field_r, vals in self._read_particle_fields(chunks, ptf, selector):\n",
       "?                                                         ^^^^^^     ----------\n",
       "\n",
       "+         for field_r, vals in self._read_particle_fields(dobj, ptf):\n",
       "?                                                         ^^^^\n",
       "\n",
       "              # Note that we now need to check the mappings\n",
       "              for field_f in field_maps[field_r]:\n",
       "                  my_ind = ind[field_f]\n",
       "-                 # mylog.debug(\"Filling %s from %s to %s with %s\",\n",
       "-                 #    field_f, my_ind, my_ind+vals.shape[0], field_r)\n",
       "                  rv[field_f][my_ind : my_ind + vals.shape[0], ...] = vals\n",
       "                  ind[field_f] += vals.shape[0]\n",
       "          # Now we need to truncate all our fields, since we allow for\n",
       "          # over-estimating.\n",
       "          for field_f in ind:\n",
       "              rv[field_f] = rv[field_f][: ind[field_f]]\n",
       "          return rv\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962f121-3fff-4647-b8ae-24dc7b9bdb48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
